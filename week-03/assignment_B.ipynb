{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgz49PvHhLt"
      },
      "source": [
        "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
        "\n",
        "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1LqgujQUbv6X"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets\n",
        "# tqdm : 반복문의 진행 상황을 시각적으로 표시하는 progress bar를 제공\n",
        "# boto3 : Amazon Web Services (AWS) 에 접근하기 위한 SDK\n",
        "# requests : HTTP 요청을 보내고 응답을 받는 라이브러리\n",
        "# regex : 정규 표현식을 사용하여 텍스트를 처리하는 라이브러리\n",
        "# sentencepiece : Google에서 개발한 SentencePiece 모델을 사용하여 텍스트를 subword 단위로 분할하는 라이브러리\n",
        "# sacremoses : 자연어 처리에서 흔히 사용되는 토큰화, stemming, lemmatization 등의 기능을 제공하는 라이브러리\n",
        "# datasets : Hugging Face에서 제공하는 Datasets 라이브러리로, 다양한 자연어 처리 데이터셋을 쉽게 다운로드 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YP3FxG9IF7O"
      },
      "source": [
        "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lGiZUoPby6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DistilBERT tokenizer -> GPT tokenizer\n",
        "# tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'openai-gpt')\n",
        "tokenizer.pad_token = tokenizer.unk_token # padding token 추가\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Hugging Face Transformers 라이브러리에서 DistilBERT 모델의 토크나이저를 로드하는 코드.\n",
        "토크나이저는 텍스트를 모델이 이해할 수 있는 숫자 시퀀스로 변환하는 역할.\n",
        "\n",
        "용도:\n",
        "텍스트 데이터를 DistilBERT 모델에 입력하기 전에 전처리하는 데 사용.\n",
        "텍스트를 토큰으로 분할하고 각 토큰에 해당하는 ID를 부여.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvfl_uFLIMWO"
      },
      "source": [
        "DistilBERT의 tokenizer를 불러왔으면 이제 `collate_fn`과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "요약:\n",
        "\n",
        "IMDB 데이터셋을 로드하고, 텍스트 데이터를 DistilBERT 토크나이저로 처리하여\n",
        "모델 입력에 적합한 형태로 변환하는 collate_fn 함수를 정의하고, 데이터 로더를 생성하는 코드.\n",
        "\n",
        "용도:\n",
        "\n",
        "IMDB 데이터셋을 Hugging Face Datasets 라이브러리를 사용하여 로드함.\n",
        "collate_fn 함수를 정의하여 텍스트 데이터를 일괄 처리하고, 토크나이징, 패딩, 잘라내기를 수행하여 모델 입력에 적합한 텐서 형태로 변환함.\n",
        "DataLoader를 사용하여 학습 및 테스트 데이터 로더를 생성함.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "ds = load_dataset(\"fancyzhx/ag_news\")\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  # del: max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['text'])\n",
        "\n",
        "  #texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF34XkoYIeEm"
      },
      "source": [
        "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJaUp2Vob0U-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "요약:\n",
        "Hugging Face Transformers 라이브러리에서 DistilBERT 모델을 로드하는 코드\n",
        "torch.hub.load() 함수를 사용해서 미리 학습된 DistilBERT 모델을 가져오고 출력.\n",
        "\n",
        "용도:\n",
        "텍스트 분류 등 다양한 자연어 처리 작업에 사용할 수 있는 미리 학습된 DistilBERT 모델을 로드.\n",
        "모델 아키텍처를 확인하고, 추가적인 fine-tuning 또는 수정을 위한 기반으로 사용할 수 있음.\n",
        "\"\"\"\n",
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh-tqY8WInQt"
      },
      "source": [
        "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
        "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
        "\n",
        "---\n",
        "\n",
        "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW7ETZQzzNp2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  \"\"\"\n",
        "  GPT 모델의 경우, 입력 텍스트의 마지막 토큰이 텍스트 전체의 의미를 가장 잘 담고 있으므로,\n",
        "  마지막 토큰의 representation을 사용하여 분류하는 것이 더 적합.\n",
        "\n",
        "  .'. forward 메서드를 수정하여\n",
        "  마지막 토큰의 representation을 사용하도록 변경\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'openai-gpt')\n",
        "\n",
        "    self.classifier = nn.Linear(768, 4) # 출력 차원 4\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)['last_hidden_state']\n",
        "    # x = self.classifier(x[:, 0])\n",
        "    x = self.classifier(x[:, -1]) # 마지막 토큰의 representation 사용\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hFvSis0JLju"
      },
      "source": [
        "위와 같이 `TextClassifier`의 `encoder`를 불러온 DistilBERT, 그리고 `classifier`를 linear layer로 설정합니다.\n",
        "그리고 `forward` 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
        "\n",
        "---\n",
        "\n",
        "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "요약\n",
        "모델의 인코더 부분 가중치를 고정하여 학습되지 않도록 설정.\n",
        "\n",
        "용도\n",
        "전이 학습에서 모델의 인코더 부분은 이미 잘 학습되어 있으므로,\n",
        "새 데이터에 맞게 마지막 분류기 레이어만 학습하기 위해 사용.\n",
        "\"\"\"\n",
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU7BWEbgJeKm"
      },
      "source": [
        "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
        "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
        "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
        "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
        "\n",
        "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "4f4478b0-5cb6-4282-a91e-614527b3d705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 723720.899017334\n",
            "Epoch   1 | Train Loss: 722869.1695251465\n",
            "Epoch   2 | Train Loss: 722794.111541748\n",
            "Epoch   3 | Train Loss: 722767.2051086426\n",
            "Epoch   4 | Train Loss: 722811.3719329834\n",
            "Epoch   5 | Train Loss: 722888.4775085449\n",
            "Epoch   6 | Train Loss: 722723.2508544922\n",
            "Epoch   7 | Train Loss: 723008.5843200684\n",
            "Epoch   8 | Train Loss: 722803.3864746094\n",
            "Epoch   9 | Train Loss: 722864.1385803223\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "# loss_fn = nn.BCEWithLogitsLoss() # 이진 분류를 위한 손실 함수입니다. (주석 처리됨)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중 클래스 분류를 위한 손실 함수를 설정합니다.\n",
        "\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr) # Adam 옵티마이저를 생성하고, 모델의 파라미터와 학습률을 전달합니다.\n",
        "n_epochs = 10 # 학습할 에포크 횟수를 10으로 설정합니다.\n",
        "\n",
        "for epoch in range(n_epochs):               # 설정된 에포크 횟수만큼 반복합니다.\n",
        "  total_loss = 0.                           # 에포크마다 총 손실을 0으로 초기화합니다.\n",
        "  model.train()                             # 모델을 학습 모드로 설정합니다.\n",
        "  for data in train_loader:                 # train_loader에서 배치 단위로 데이터를 가져옵니다.\n",
        "    model.zero_grad()                       # 이전 배치의 그래디언트를 초기화합니다.\n",
        "    inputs, labels = data                   # 배치 데이터를 입력과 레이블로 분리합니다.\n",
        "    # inputs, labels = inputs.to('cuda'), labels.to('cuda').float() # 입력과 레이블을 GPU로 이동시킵니다.\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda') # .float() 제거\n",
        "\n",
        "\n",
        "    # preds = model(inputs)[..., 0]           # 모델에 입력을 넣어 예측값을 계산합니다.\n",
        "    preds = model(inputs)\n",
        "\n",
        "    loss = loss_fn(preds, labels)           # 예측값과 레이블을 사용하여 손실을 계산합니다.\n",
        "    loss.backward()                         # 손실을 기반으로 역전파를 수행하여 그래디언트를 계산합니다.\n",
        "    optimizer.step()                        # 계산된 그래디언트를 사용하여 모델의 가중치를 업데이트합니다.\n",
        "\n",
        "    total_loss += loss.item()               # 배치 손실을 총 손실에 누적합니다.\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\") # 에포크 번호와 총 손실을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DjphVwXL00E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84593d4d-9039-4f15-88a9-f669f1d624b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========> Train acc: 0.250 | Test acc: 0.250\n"
          ]
        }
      ],
      "source": [
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  train_acc = accuracy(model, train_loader)\n",
        "  test_acc = accuracy(model, test_loader)\n",
        "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfFUkEM1ZWeG"
      },
      "source": [
        "Loss가 잘 떨어지고, 이전에 우리가 구현한 Transformer보다 더 빨리 수렴하는 것을 알 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DJN8W-7q5gO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}